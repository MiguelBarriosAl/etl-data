{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo 1: lectura, tratamiento y carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import re, glob, os, math\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Lectura de datos\n",
    "\n",
    "En este caso los datos se pueden descargar directamente en formato .zip desde la página oficial para poder trabajar con ellos desde manera local.\n",
    "* (https://datos.madrid.es/portal/site/egob/menuitem.c05c1f754a33a9fbe4b2e4b284f1a5a0/?vgnextoid=41e01e007c9db410VgnVCM2000000c205a0aRCRD&vgnextchannel=374512b9ace9f310VgnVCM100000171f5a0aRCRD&vgnextfmt=default\n",
    "\n",
    "En el caso que se encontraran solo dispuestos a través de la url, se haría uso de la librería 'urllib3'. Esta librería te permite conectarte a la página y hacer scraping en los datos.\n",
    "\n",
    "Tras revisar como están dispuestos los datos, elaboro 3 funciones en el que permita extraer los ficheros en sus diferentes formatos(.txt, .csv, .xml).\n",
    "Puede ser útil para en un futuro se decide cambiar según su formato al inicial.\n",
    "\n",
    "Para esta prueba técnica he elegido el formato .csv  para mostrar los diferentes pasos de ETL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_txt_file():\n",
    "    '''Retrieve file '''\n",
    "    txt_files=[]\n",
    "    zip_ref = zipfile.ZipFile('Anio201810.zip','r')\n",
    "    paths = zip_ref.namelist()\n",
    "    for path in paths:\n",
    "        pattern = re.compile(r'\\b.txt\\b')\n",
    "        reTXT = pattern.search(path)\n",
    "        if reTXT is not None:\n",
    "            file=zip_ref.extract(path)\n",
    "            txt_files.append(file)    \n",
    "    return txt_files\n",
    "\n",
    "def get_csv_file():\n",
    "    '''Retrieve file '''\n",
    "    csv_files=[]\n",
    "    zip_ref = zipfile.ZipFile('Anio201810.zip','r')\n",
    "    paths = zip_ref.namelist()\n",
    "    for path in paths:\n",
    "        pattern = re.compile(r'\\b.csv\\b')\n",
    "        reCSV = pattern.search(path)\n",
    "        if reCSV is not None:\n",
    "            file=zip_ref.extract(path)\n",
    "            csv_files.append(file)    \n",
    "    return csv_files\n",
    "\n",
    "def get_xml_file():\n",
    "    '''Retrieve file '''\n",
    "    xml_files=[]\n",
    "    zip_ref = zipfile.ZipFile('Anio201810.zip','r')\n",
    "    paths = zip_ref.namelist()\n",
    "    for path in paths:\n",
    "        pattern = re.compile(r'\\b.xml\\b')\n",
    "        reXML = pattern.search(path)\n",
    "        if reXML is not None:\n",
    "            file=zip_ref.extract(path)\n",
    "            xml_files.append(file)    \n",
    "    return xml_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Lectura y tratamiento de datos\n",
    "Para este apartado se va a proceder a la carga de los ficheros .csv a un dataframe, en el que se pretende hacer los siguientes pasos por orden:\n",
    "\n",
    "* Lectura de los ficheros .csv a dataframe\n",
    "* Concatenación de los dataframes\n",
    "* Ordenar de manera ascendente según el mes y día de mes\n",
    "* Revisión que no se pierdan datos durante el proceso\n",
    "\n",
    "Es interesante para este caso ordenar los datos según el mes y día de manera ascendete para posteriores análisis y represantciones gráficas y así cononocer la evolución de las concentraciones de NO2\n",
    "\n",
    "Se divide el proceso en dos funciones diferentes:\n",
    "\n",
    "* Función donde se realiza la lectura, concatenación y ordenar los datos (processing_data)\n",
    "* Función para comprobar que durante el proceso no se hayan perdido datos en el camino (check_processing_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_data():\n",
    "    '''Concatenate dataframe and sort'''\n",
    "    \n",
    "    df1=pd.DataFrame()\n",
    "    csv_file=get_csv_file()\n",
    "    for path in csv_file:\n",
    "        df = pd.read_csv(path, delimiter=\";\")\n",
    "        if not df1.empty:\n",
    "            df=pd.concat([df, df1])\n",
    "        df1=df\n",
    "    df = df1.sort_values(['MES','DIA'], ascending=(True,True))   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante comprobar que se haya hecho la carga de los ficheros en dataframe de manera satisfactoria y no se haya perdido alguno en el camino.\n",
    "Para ello elaboro una función donde compruebe la suma de las filas por cada fichero sea igual al 'df'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Satisfactory process\n"
     ]
    }
   ],
   "source": [
    "def check_processing_data():\n",
    "    \n",
    "    array_rows=[]\n",
    "    csv_file=get_csv_file()\n",
    "    total_df_rows=processing_data()\n",
    "    total_df_rows=total_df_rows.shape[0]\n",
    "    '''Sum row files'''\n",
    "    for path in csv_file:\n",
    "        dataframe = pd.read_csv(path, delimiter=\";\")\n",
    "        rows=dataframe.shape[0]\n",
    "        array_rows=array_rows+[rows]\n",
    "    total_array_rows=np.sum(array_rows)\n",
    "    '''Compare row numbers df and row files'''\n",
    "    if total_df_rows==total_array_rows:\n",
    "        print('Satisfactory process')\n",
    "    else:\n",
    "        print('Something went wrong')\n",
    "    \n",
    "check_processing_data()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Data Wrangling: Tratamiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este apartado se pretende hacer un tratamiento de datos para tener solo aquellos que nos aporte valor a la hora de realizar análisis de NO2 y representaciones gráficas.\n",
    "\n",
    "Para ello este apartado se divide en dos partes:\n",
    "\n",
    "1. Filtrado de datos NO2 y valores válidos\n",
    "2. Filtrado de valores Null y valores Nan\n",
    "\n",
    "A su vez estará compuesto por 3 funciones:\n",
    "\n",
    "* Función de filtrado NO2 y valores válidos (filtered_data)\n",
    "* Función filtrado valores Null\n",
    "* Función filtrado valores Nan\n",
    "\n",
    "Funcion específica de python (pd.isnull), que devuelve un vector en colummnas con valores True/False, donde True indica que es null, False no es null\n",
    "\n",
    "Para este ejercicio haré uso de la función pd.isnull/pd.isnan en el filtrado de valores Null y valores Nan\n",
    "\n",
    "Nota: La existencia de valores null se puede dar por varias razones; a la hora de extraer los datos de la base de datos puede haber incompatibilidades y en la recolección de los datos antes de guardarse en la base de datos, represantandose como NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtered_data():\n",
    "    '''Filtering NO2 Data'''\n",
    "    pattern = \"(_8_)\"\n",
    "    filtered = df['PUNTO_MUESTREO'].str.extract(pattern)\n",
    "    result = pd.concat([df, filtered], axis=1, sort=False)\n",
    "    df_NO2=result[result[0]=='_8_']\n",
    "    '''Filtering NO2 Data'''\n",
    "    valid=['V01','V02','V03','V04','V05','V06', 'V07','V08','V09',\n",
    "           'V10','V11', 'V12', 'V13', 'V14','V15','V16','V17','V18',\n",
    "           'V19','V20','V21','V22','V23','V24']\n",
    "    for v in valid:\n",
    "        dataframe=df_NO2[df_NO2[v]=='V']\n",
    "        \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He tomado la decisión de eliminar los registros no válidos (aquellas filas con un registro no valido:'N') tras comprobar que no eran demasiados(aprox 25). Por lo que he sopesado que es más beneficioso obtener solo valores válidos que mantenerlos para conseguir resultados más ajustados a la realidad. \n",
    "En el caso que hubiera demasiados datos no válidos que supusiera la eliminación de demasiados registros, se debería plantear la posibilidad de sustituir esos registros por otros aquellos que no influyera demasiado en los resultados, ya que evitaría el quedarnos con pocos.\n",
    "\n",
    "Por otro lado en la función he decidido hacer una iteración de un array(valid) escrito a mano los campos, ya que al ser una prueba técnica he preferido no cargar de más código. En otra ocasión que hubiera más atributos y fuera un trabajo muy mecánico, se podría conseguir el array a través de otros métodos como las expresiones regulares que se han ido utilizando .\n",
    "\n",
    "En la siguiente dos funciones se procede a revisar la existencia de valores Null/Nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are not NaN values\n",
      "There are not Null values\n"
     ]
    }
   ],
   "source": [
    "def check_values_null():\n",
    "    df = processing_data()\n",
    "    df_null=df.isnull().values.ravel().sum()\n",
    "    if df_null==0:\n",
    "        print('There are not Null values')\n",
    "    else:\n",
    "        print(f'There are {df_null} null values')\n",
    "\n",
    "def check_values_nan():\n",
    "    df = processing_data()\n",
    "    df_nan=df.isna().values.ravel().sum()\n",
    "    if df_nan==0:\n",
    "        print('There are not NaN values')\n",
    "    else:\n",
    "        print(f'There are {df_nan} NaN values')\n",
    "        \n",
    "check_values_nan()\n",
    "check_values_null()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Almacenamiento de datos\n",
    "Para esta situación donde no hay un excesivo número de registros en el dataframe final, se procede a crear una función donde se almacene en un fichero .csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe=filtered_data()\n",
    "file_name='CargaBD/NO2_data.csv'\n",
    "dataframe.to_csv(file_name, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 APUNTES\n",
    "\n",
    "* En una situación real, en la función de descompresión de los .zip, se debería de hacer una función de limpieza de los mismos ficheros tras finalizar el proceso de ETL, para mantener las carpetas limpias y en orden, a no ser que por otras razones no interese. \n",
    "* También en una situación donde se está en contacto directo con los demás departamentos, se debería de hacer una limpieza más exhaustiva. Cada proceso de tratamiento de datos, deben de ir acompañados de funciones donde revise que no haya pérdidas de datos durante el camino.Para este ejercicio práctico lo he realizado tras la carga de datos para mostrar la importancia de este proceso. Los siguientes apartados los he realizado de manera individual sin mostrar las funciones para centrarse en los demás procesos.\n",
    "* He querido dividir el ejercicio en diferentes funciones con el fin de hacerlo más modular, además de intentar conseguir un código más limpio y claro de leer.\n",
    "* A nivel personal no me gusta cargar de demasiados comentarios los códigos. Siempre intento de hacer un número razonable de comentarios, ya que si junto con el código se consigue un código limpio y facil de entender, es una buena muestra de conseguir buenas prácticas de codificación.\n",
    "* En la carga de datos he hecho una simple carga de datos a .csv porque he considerado que no es un número excesivo de registro. En otras situaciones que se prefiera hacer carga a una base de datos, python permite hacer conexiones a base de datos y posteriormente carga de los datos.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
